{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sns.set_style(\"white\")  # change le style par défaut des graphiques seaborn\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/heart_2020_cleaned.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rééquilibrage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heart_disease = df[df['HeartDisease'] == \"Yes\"]\n",
    "df_no_heart_disease = df[df['HeartDisease'] == \"No\"]\n",
    "\n",
    "drop_indices = np.random.choice(df_no_heart_disease.index, size=len(\n",
    "    df_no_heart_disease)-len(df_heart_disease), replace=False)\n",
    "df_no_heart_disease = df_no_heart_disease.drop(drop_indices)\n",
    "\n",
    "df_new = pd.concat([df_heart_disease, df_no_heart_disease])\n",
    "df = df_new\n",
    "df_new.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean (i hope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Yes/No to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_change = [\"HeartDisease\", \"Smoking\", \"AlcoholDrinking\", \"Stroke\",\n",
    "                    \"DiffWalking\", \"Diabetic\", \"PhysicalActivity\", \"Asthma\", \"KidneyDisease\", \"SkinCancer\"]\n",
    "d = dict()\n",
    "for c in column_to_change:\n",
    "    d[c] = {\"No\": 0, \"Yes\": 1, \"No, borderline diabetes\": 2,\n",
    "            \"Yes (during pregnancy)\": 3}\n",
    "df = df.replace(d)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change categoricals columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_orig = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can find 4 more categorical variables which are not yes/no. The dtype is object for these 4. Let us convert them into dummy variables.\n",
    "categoricals = df.select_dtypes(include=['object'])\n",
    "categoricals.head()\n",
    "cat_dummies = pd.get_dummies(categoricals)\n",
    "cat_dummies.head()\n",
    "# Drop the redundant columns\n",
    "df.drop(list(categoricals.columns), axis=1, inplace=True)\n",
    "# concat the heart and dummies data frames.\n",
    "df = pd.concat([df, cat_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With categorical data splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(heart_orig.corr(), annot=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are interested in the chance of having the heart disease based on other variables.\n",
    "# Let y be HeartDisease column\n",
    "y = df.pop('HeartDisease')\n",
    "X = df\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.7, test_size=0.3, random_state=100)\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.7, test_size=0.3, random_state=100)\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale X variables - should have done before spliting\n",
    "from sklearn.preprocessing import scale\n",
    "# rescale the features of Xtrain\n",
    "cols = X_train.columns\n",
    "X_train = pd.DataFrame(scale(X_train))\n",
    "X_train.columns = cols\n",
    "# rescale the features of Xtest\n",
    "cols = X_test.columns\n",
    "X_test = pd.DataFrame(scale(X_test))\n",
    "X_test.columns = cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# not optimized models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classifier with Bagging and Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Init classifier\n",
    "xgb_cl = xgb.XGBClassifier()\n",
    "\n",
    "# The baggging ensemble classifier is initialized with:\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=xgb_cl,\n",
    "                            n_estimators=5, max_samples=50, bootstrap=True)\n",
    "\n",
    "# Training\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating\n",
    "print(f\"Train score: {bagging.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {bagging.score(X_test, y_test)}\")\n",
    "\n",
    "# Fit\n",
    "xgb_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "preds = xgb_cl.predict(X_test)\n",
    "\n",
    "# Score\n",
    "accuracy_score(y_test, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# The base learner will be a decision tree with depth = 2\n",
    "xgb_cl = xgb.XGBClassifier()\n",
    "\n",
    "adaboost = AdaBoostClassifier(\n",
    "    base_estimator=xgb_cl, n_estimators=5, learning_rate=0.1, random_state=23)\n",
    "\n",
    "# Train!\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "print(f\"Train score: {adaboost.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {adaboost.score(X_test, y_test)}\")\n",
    "\n",
    "# Fit\n",
    "xgb_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "preds = xgb_cl.predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier with Bagging and Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Import Decision Tree Classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=23)\n",
    "\n",
    "# The baggging ensemble classifier is initialized with:\n",
    "\n",
    "bagging = BaggingClassifier(\n",
    "    base_estimator=tree, n_estimators=5, max_samples=50, bootstrap=True)\n",
    "\n",
    "# Training\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating\n",
    "print(f\"Train score: {bagging.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {bagging.score(X_test, y_test)}\")\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "tree = tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# The base learner will be a decision tree with depth = 2\n",
    "tree = DecisionTreeClassifier(max_depth=2, random_state=23)\n",
    "\n",
    "adaboost = AdaBoostClassifier(\n",
    "    base_estimator=tree, n_estimators=5, learning_rate=0.1, random_state=23)\n",
    "\n",
    "# Train!\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "print(f\"Train score: {adaboost.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {adaboost.score(X_test, y_test)}\")\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "tree = tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Gaussian Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = gnb.predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"Accuracy : \", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Neighbours Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors=8)\n",
    "# knn.fit(X_train, y_train)\n",
    "# y_pred = knn.predict(X_test)\n",
    "# print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "classifierRandomForest = RandomForestClassifier(\n",
    "    max_depth=2, n_estimators=2, random_state=100, criterion='entropy')\n",
    "\n",
    "\n",
    "def calculate_accuracy(classifier, X_train, X_test, y_train, y_test, modelName):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "    accuracy_train = metrics.accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = metrics.accuracy_score(y_test, y_pred_test)\n",
    "    print(modelName, 'Train accuracy:', '{:.3f}'.format(\n",
    "        accuracy_train), 'Test accuracy', '{:.3f}'.format(accuracy_test))\n",
    "    return accuracy_train, accuracy_test, classifier\n",
    "\n",
    "\n",
    "accuracy_train, accuracy_test, trained_classifier = calculate_accuracy(\n",
    "    classifierRandomForest, X_train, X_test, y_train, y_test, modelName=\"Random Forest\")\n",
    "metrics.ConfusionMatrixDisplay.from_estimator(\n",
    "    trained_classifier, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which models are promising?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us use RFE to check required features and remove multicolearity\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Running RFE with the output number of the variable equal to 10\n",
    "lm = LogisticRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "rfe = RFE(lm, n_features_to_select=30)             # running RFE\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "l = sorted(list(zip(X_train.columns,rfe.support_,rfe.ranking_)))\n",
    "\n",
    "# sort l by rfe.ranking_\n",
    "l.sort(key=lambda x: x[1], reverse=True)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_columns = X_train.columns[rfe.support_]\n",
    "rfe_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X_train dataframe with RFE selected variables\n",
    "X_train_rfe = X_train[rfe_columns]\n",
    "X_train_rfe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Init classifier\n",
    "xgb_cl = xgb.XGBClassifier()\n",
    "\n",
    "# The baggging ensemble classifier is initialized with:\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=xgb_cl,\n",
    "                            n_estimators=5, max_samples=50, bootstrap=True)\n",
    "\n",
    "# Training\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating\n",
    "print(f\"Train score: {bagging.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {bagging.score(X_test, y_test)}\")\n",
    "\n",
    "# Fit\n",
    "xgb_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "preds = xgb_cl.predict(X_test)\n",
    "\n",
    "# Score\n",
    "accuracy_score(y_test, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# The base learner will be a decision tree with depth = 2\n",
    "xgb_cl = xgb.XGBClassifier()\n",
    "\n",
    "adaboost = AdaBoostClassifier(\n",
    "    base_estimator=xgb_cl, n_estimators=5, learning_rate=0.1, random_state=23)\n",
    "\n",
    "# Train!\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "print(f\"Train score: {adaboost.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {adaboost.score(X_test, y_test)}\")\n",
    "\n",
    "# Fit\n",
    "xgb_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "preds = xgb_cl.predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Import Decision Tree Classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=23)\n",
    "\n",
    "# The baggging ensemble classifier is initialized with:\n",
    "\n",
    "bagging = BaggingClassifier(\n",
    "    base_estimator=tree, n_estimators=5, max_samples=50, bootstrap=True)\n",
    "\n",
    "# Training\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating\n",
    "print(f\"Train score: {bagging.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {bagging.score(X_test, y_test)}\")\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "tree = tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# The base learner will be a decision tree with depth = 2\n",
    "tree = DecisionTreeClassifier(max_depth=2, random_state=23)\n",
    "\n",
    "adaboost = AdaBoostClassifier(\n",
    "    base_estimator=tree, n_estimators=5, learning_rate=0.1, random_state=23)\n",
    "\n",
    "# Train!\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "print(f\"Train score: {adaboost.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {adaboost.score(X_test, y_test)}\")\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "tree = tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Gaussian Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = gnb.predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"Accuracy : \", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "classifierRandomForest = RandomForestClassifier(\n",
    "    max_depth=2, n_estimators=2, random_state=100, criterion='entropy')\n",
    "\n",
    "\n",
    "def calculate_accuracy(classifier, X_train, X_test, y_train, y_test, modelName):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "    accuracy_train = metrics.accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = metrics.accuracy_score(y_test, y_pred_test)\n",
    "    print(modelName, 'Train accuracy:', '{:.3f}'.format(\n",
    "        accuracy_train), 'Test accuracy', '{:.3f}'.format(accuracy_test))\n",
    "    return accuracy_train, accuracy_test, classifier\n",
    "\n",
    "\n",
    "accuracy_train, accuracy_test, trained_classifier = calculate_accuracy(\n",
    "    classifierRandomForest, X_train, X_test, y_train, y_test, modelName=\"Random Forest\")\n",
    "metrics.ConfusionMatrixDisplay.from_estimator(\n",
    "    trained_classifier, X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "963220819e2030a8d1a886738a7482cc0fc836200a0d406e08d73d51abd24cb5"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
