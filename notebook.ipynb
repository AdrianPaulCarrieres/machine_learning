{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "sns.set_style(\"white\") # change le style par d√©faut des graphiques seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/heart_2020_cleaned.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean (i hope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Yes/No to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_change = [\"HeartDisease\", \"Smoking\", \"AlcoholDrinking\", \"Stroke\", \"DiffWalking\", \"Diabetic\", \"PhysicalActivity\", \"Asthma\", \"KidneyDisease\", \"SkinCancer\"]\n",
    "d = dict()\n",
    "for c in column_to_change:\n",
    "    d[c] = {\"No\": 0, \"Yes\": 1, \"No, borderline diabetes\": 0, \"Yes (during pregnancy)\": 0}\n",
    "df = df.replace(d)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change categoricals columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_orig = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can find 4 more categorical variables which are not yes/no. The dtype is object for these 4. Let us convert them into dummy variables.\n",
    "categoricals = df.select_dtypes(include=['object'])\n",
    "categoricals.head()\n",
    "cat_dummies = pd.get_dummies(categoricals)\n",
    "cat_dummies.head()\n",
    "# Drop the redundant columns\n",
    "df.drop(list(categoricals.columns), axis=1, inplace=True)\n",
    "# concat the heart and dummies data frames.\n",
    "df = pd.concat([df, cat_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With categorical data splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "sns.heatmap(df.corr(),annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "sns.heatmap(heart_orig.corr(),annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are interested in the chance of having the heart disease based on other variables.\n",
    "# Let y be HeartDisease column\n",
    "y = df.pop('HeartDisease')\n",
    "X = df\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale X variables - should have done before spliting\n",
    "from sklearn.preprocessing import scale\n",
    "# rescale the features of Xtrain\n",
    "cols = X_train.columns\n",
    "X_train = pd.DataFrame(scale(X_train))\n",
    "X_train.columns = cols\n",
    "# rescale the features of Xtest\n",
    "cols = X_test.columns\n",
    "X_test = pd.DataFrame(scale(X_test))\n",
    "X_test.columns = cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# not optimized models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classifier with Bagging and Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Init classifier\n",
    "xgb_cl = xgb.XGBClassifier()\n",
    "\n",
    "# The baggging ensemble classifier is initialized with:\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=xgb_cl, n_estimators=5, max_samples=50, bootstrap=True)\n",
    "\n",
    "# Training\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating\n",
    "print(f\"Train score: {bagging.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {bagging.score(X_test, y_test)}\")\n",
    "\n",
    "# Fit\n",
    "xgb_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "preds = xgb_cl.predict(X_test)\n",
    "\n",
    "# Score\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "\n",
    "# The base learner will be a decision tree with depth = 2\n",
    "xgb_cl = xgb.XGBClassifier()\n",
    "\n",
    "adaboost = AdaBoostClassifier(base_estimator=xgb_cl, n_estimators=5, learning_rate=0.1, random_state=23)\n",
    "\n",
    "# Train!\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "print(f\"Train score: {adaboost.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {adaboost.score(X_test, y_test)}\")\n",
    "\n",
    "# Fit\n",
    "xgb_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "preds = xgb_cl.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier with Bagging and Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "# Import Decision Tree Classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics \n",
    "\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=23)\n",
    "\n",
    "# The baggging ensemble classifier is initialized with:\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=tree, n_estimators=5, max_samples=50, bootstrap=True)\n",
    "\n",
    "# Training\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating\n",
    "print(f\"Train score: {bagging.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {bagging.score(X_test, y_test)}\")\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "tree = tree.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# The base learner will be a decision tree with depth = 2\n",
    "tree = DecisionTreeClassifier(max_depth=2, random_state=23)\n",
    "\n",
    "\n",
    "adaboost = AdaBoostClassifier(base_estimator=tree, n_estimators=5, learning_rate=0.1, random_state=23)\n",
    "\n",
    "# Train!\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "print(f\"Train score: {adaboost.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {adaboost.score(X_test, y_test)}\")\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "tree = tree.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = gnb.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "print (\"Accuracy : \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Neighbours Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    " \n",
    "# knn = KNeighborsClassifier(n_neighbors=8)\n",
    "# knn.fit(X_train, y_train)\n",
    "# y_pred = knn.predict(X_test)\n",
    "# print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which models are promising?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "963220819e2030a8d1a886738a7482cc0fc836200a0d406e08d73d51abd24cb5"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
